{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Please enter your **name, surname** and **student number** instead of `\"NAME-HERE\"`, `\"SURNAME-HERE\"`, `\"NUMBER-HERE\"` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Esra', 'surname': 'Sekerci', 'studentNumber': '2698125'}\n"
     ]
    }
   ],
   "source": [
    "student = {\n",
    "    'name' : \"Esra\" ,\n",
    "    'surname' : \"Sekerci\", \n",
    "    'studentNumber' : \"2698125\"\n",
    "}\n",
    "\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement the gradient descent algorithm and apply it for Support Vector Machine (SVM) classification and linear regression.\n",
    "\n",
    "As a reminder, the gradient descent algorithm is shown below\n",
    "\n",
    "---\n",
    "\n",
    "- Start from an initial point $\\pmb{x}^{(0)} \\in \\mathbb{R}^d$, $t=0$\n",
    "- **for** $i = 1, \\dots, m$ \n",
    "  - $\\pmb{x}^{(i)}=\\pmb{x}^{(i-1)} - \\alpha(i) \\nabla_{\\pmb x}f(\\pmb{x}^{(i-1)})$\n",
    "- **return** $\\pmb{x}^{(i)}$\n",
    "\n",
    "---\n",
    "\n",
    "## Some helper functions\n",
    "\n",
    "Below are some helper functions to generate *row vectors* and *column vectors*.\n",
    "\n",
    "`rv` function returns a numpy row vector, when you pass it a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv(value_list):\n",
    "   return np.array([value_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv([5,4,3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cv` function returns a numpy column vector, when you pass it a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(value_list):\n",
    "   return np.transpose(rv(value_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [4],\n",
       "       [3],\n",
       "       [2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv([5,4,3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use column vectors a lot for our gradient descent algorithm, so you will use the `cv` function. \n",
    "\n",
    "## Generic Gradient Descent\n",
    "\n",
    "You will first implement gradient descent `gdescent`as a function that has the following inputs\n",
    "\n",
    "-  `f` is a function that takes a column vector `x` as the input and returns a real number.\n",
    "- `df` is a function that takes a column vector `x` as the input and returns a column vector which is the gradient of `f` at `x`\n",
    "- `m` is the maximum number of iterations\n",
    "- `alpha` is a *function* that takes the iteration index as its input and returns a step size\n",
    "- `x0` is a column vector that is the initial value of $x$\n",
    "\n",
    "`gdescent` function will return\n",
    "\n",
    "- `x` the value at the final step\n",
    "- `flist` list of values of `f` found during all iterations, including `f(x0)`\n",
    "- `xs` list of values of `x` found during all iterations, including `x0`\n",
    "\n",
    "#### 50 points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdescent(f, df, x0, m, alpha):\n",
    "    x = x0\n",
    "    flist = [f(x0)]  # Initialize flist with the initial function value\n",
    "    xs = [x0]  # Initialize xs with the initial point\n",
    "\n",
    "    for i in range(m):\n",
    "        step_size = alpha(i)\n",
    "        x = x - step_size * df(x)\n",
    "        flist.append(f(x))\n",
    "        xs.append(x)\n",
    "\n",
    "    return x, flist, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases\n",
    "\n",
    "You can test your `gdescent` function by using the `f` and `df`s below.\n",
    "\n",
    "`f1` is $f(x) = (3x+3)^2$, its derivative `df1` is $f'(x)=3 \\times 2\\times(3x+3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return float((3 * x + 3)**2)\n",
    "\n",
    "def df1(x):\n",
    "    return 3 * 2 * (3 * x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f2` is $f(x, y) = (x - 2)(x - 3)(x + 3)(x + 1)+(x + y - 1)^2$, its gradient `df1` is\n",
    "\n",
    " $$\\nabla f=\\begin{bmatrix} (x-3)(x+3)(x+1) +  (x-2)(x+3)(x+1) + (x-2)(x-3)(x+1) + (x-2)(x-3)(x+3) + 2(x+y+-1) \\\\ 2(x + y -1) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(v):\n",
    "    x = float(v[0]); y = float(v[1])\n",
    "    return (x - 2.) * (x - 3.) * (x + 3.) * (x + 1.) + (x + y -1)**2\n",
    "\n",
    "def df2(v):\n",
    "    x = float(v[0]); y = float(v[1])\n",
    "    return cv([(-3. + x) * (-2. + x) * (1. + x) + \\\n",
    "               (-3. + x) * (-2. + x) * (3. + x) + \\\n",
    "               (-3. + x) * (1. + x) * (3. + x) + \\\n",
    "               (-2. + x) * (1. + x) * (3. + x) + \\\n",
    "               2 * (-1. + x + y),\n",
    "               2 * (-1. + x + y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can use the `package_ans` function below, which returns the `x` from the last iteration, and the first and last values in `fs` and `xs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_ans(vals):\n",
    "    x, fs, xs = vals\n",
    "    return [x.tolist(), [fs[0], fs[-1]], [xs[0].tolist(), xs[-1].tolist()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two test cases that you can test your function with `f1`and `f2`. You can modify them or define more test cases if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1\n",
      "[[[-0.9999999999999997]], [9.0, 7.888609052210118e-31], [[[0.0]], [[-0.9999999999999997]]]]\n",
      "Test Case 2\n",
      "[[[-2.2058239041648853], [3.205823890926977]], [19.0, -20.967239611348752], [[[0.0], [0.0]], [[-2.2058239041648853], [3.205823890926977]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrah\\AppData\\Local\\Temp\\ipykernel_15412\\2810906081.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float((3 * x + 3)**2)\n",
      "C:\\Users\\ibrah\\AppData\\Local\\Temp\\ipykernel_15412\\2066832812.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  x = float(v[0]); y = float(v[1])\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "ans=package_ans(gdescent(f1, df1, cv([0.]), 1000, lambda i: 0.1))\n",
    "print('Test Case 1')\n",
    "print(ans)\n",
    "\n",
    "# Test case 2\n",
    "ans=package_ans(gdescent(f2, df2, cv([0., 0.]), 1000, lambda i: 0.01))\n",
    "print('Test Case 2')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: You need to implement step size `alpha` as a function in your model. So it should be called as `alpha(i)` where `i` is the current iteration. Note that in the test cases above we pass an anonymous function `lambda i : 0.1` for this. So the `alpha` function in your model gets the argument `i` and returns `0.1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Numeric Differentiation\n",
    "\n",
    "Calculating the gradient of functions analytically ca be difficult. And for large models, such as neural networks, computing the analytical gradients is practically impossible. \n",
    "\n",
    "A useful method for estimating the gradient is the *finite differences* approach. Suppose the we have a function $f(\\pmb{x})$ that takes a column vector $\\pmb{x}$, suppose we want to estimate the gradient of $f$ at a particular $\\pmb{x}^{(0)}$\n",
    "\n",
    "$\\nabla_xf(\\pmb{x}^{(0)})$ can be estimated as \n",
    "\n",
    "$$\\frac {\\partial f}{\\partial x_i}= \\frac {f(\\pmb{x}^{(0)} + \\delta^i )- f(\\pmb{x}^{(0)} - \\delta^i))}{2\\delta}$$\n",
    "\n",
    "where $\\delta^i$ is a column vector where $i^{th}$ coordinate is a small constant $\\delta$  such as 0.0001 all other components are $0$.\n",
    "\n",
    "For example, if $x^{(0)}=[1,1,…,1]^T$ and $\\delta=0.01$, we may approximate the first component of $\\nabla_x f(x^{(0)})$ as\n",
    "\n",
    "$$\\frac{f([1,1,1,…,1]^T + [0.01,0,0,\\dots,0])-f([1,1,1,…,1]^T - [0.01,0,0,\\dots,0])}{2\\times0.01}$$\n",
    "\n",
    "and the second component of  $\\nabla_x f(x^{(0)})$ as\n",
    "\n",
    "$$\\frac{f([1,1,1,…,1]^T + [0,0.01,0,\\dots,0])-f([1,1,1,…,1]^T - [0,0.01,0,\\dots,0])}{2\\times0.01}$$\n",
    "\n",
    "This should be done for each dimension independently, and they should be put together to form the estimated gradient. For example, if you $\\pmb x$ column vector has 5 dimensions, you calculate this independently for each of 5 dimensions, and put them together to form a resulting gradient with 5 dimensions.\n",
    "\n",
    "Implement this as a function `num_grad` that takes the function `f` and the small value `delta` ($\\delta$) as its arguments, and returns a new **function** that takes `x` and returns a gradient column vector.\n",
    "\n",
    "#### 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_grad(f, delta=0.001):\n",
    "    def df(x):\n",
    "        n = len(x)\n",
    "        grad = np.zeros((n, 1))\n",
    "        for i in range(n):\n",
    "            delta_i = np.zeros((n, 1))\n",
    "            delta_i[i] = delta\n",
    "            grad[i] = (f(x + delta_i) - f(x - delta_i)) / (2 * delta)\n",
    "        return grad\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cases\n",
    "\n",
    "Below are some test cases to use with your `num_grad` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Gradient Test Case 1\n",
      "([[18.000000000000682]], [[0.0]])\n",
      "Numerical Gradient Test Case 2\n",
      "([[19.79999999999915]], [[0.1]])\n",
      "Numerical Gradient Test Case 3\n",
      "([[6.99999899999959], [-2.000000000000668]], [[0.0], [0.0]])\n",
      "Numerical Gradient Test Case 4\n",
      "([[4.7739994000011166], [-2.000000000000668]], [[0.1], [-0.1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrah\\AppData\\Local\\Temp\\ipykernel_15412\\2810906081.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float((3 * x + 3)**2)\n"
     ]
    }
   ],
   "source": [
    "x = cv([0.])\n",
    "ans=(num_grad(f1)(x).tolist(), x.tolist())\n",
    "print('Numerical Gradient Test Case 1')\n",
    "print(ans)\n",
    "\n",
    "x = cv([0.1])\n",
    "ans=(num_grad(f1)(x).tolist(), x.tolist())\n",
    "print('Numerical Gradient Test Case 2')\n",
    "print(ans)\n",
    "\n",
    "x = cv([0., 0.])\n",
    "ans=(num_grad(f2)(x).tolist(), x.tolist())\n",
    "print('Numerical Gradient Test Case 3')\n",
    "print(ans)\n",
    "\n",
    "x = cv([0.1, -0.1])\n",
    "ans=(num_grad(f2)(x).tolist(), x.tolist())\n",
    "print('Numerical Gradient Test Case 4')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent using the Numerical Gradient\n",
    "\n",
    "Our previous implementation of gradient descent `gdescent`  above inputs both the function `f` and gradient `df`. Write a function `minimize` that only takes `f` as its input and uses this function and the numerical gradient implemented in `num_grad` above to find the optimal point. \n",
    "\n",
    ">  `minimize` function should call `num_grad` just once, and then the function that `num_grad` returns should be called many times. The outputs of the `minimize` should be the same as the `gdescent`  \n",
    "\n",
    "#### 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(f, x0, m, alpha):\n",
    "    df = num_grad(f)\n",
    "    return gdescent(f, df, x0, m, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cases\n",
    "\n",
    "Below are some test cases to use with your `minimize` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimize Test Case 1\n",
      "[[[-1.0]], [9.0, 0.0], [[[0.0]], [[-1.0]]]]\n",
      "Minimize Test Case 2\n",
      "[[[-2.2058237062057517], [3.205823692967833]], [19.0, -20.967239611347775], [[[0.0], [0.0]], [[-2.2058237062057517], [3.205823692967833]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrah\\AppData\\Local\\Temp\\ipykernel_15412\\2810906081.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float((3 * x + 3)**2)\n"
     ]
    }
   ],
   "source": [
    "x = cv([0.])\n",
    "ans = package_ans(minimize(f1, cv([0.]), 1000, lambda i: 0.1))\n",
    "print('Minimize Test Case 1')\n",
    "print(ans)\n",
    "\n",
    "x = cv([0., 0.])\n",
    "ans = package_ans(minimize(f2, cv([0., 0.]), 1000, lambda i: 0.01))\n",
    "print('Minimize Test Case 2')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We will now implement a gradient descent algorithm for linear regression. We will us the data and weights below:\n",
    "\n",
    "- `X` is the $d \\times n$ features array. We have $d=2$ features and $n=4$ examples of each of those.\n",
    "- `Y` is $1 \\times n$ output values vector\n",
    "- `w` is the initial weights\n",
    "- `w0` is the offset value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1., 2., 3., 4.], [1., 1., 1., 1.]])\n",
    "Y = np.array([[1., 2.2, 2.8, 4.1]])\n",
    "w = np.array([[1.0],[0.05]])\n",
    "w0 = np.array([[0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that returns the predictions for linear regression is shown below. This function inputs `X`, `w` and `w0` and returns the prediction for each `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(x, w, w0):\n",
    "    return np.dot(w.T, x) + w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean_squared_loss` function below computes the loss `x`, `y`, `w`, `w0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(x, y, w, w0):\n",
    "    return (y - lin_reg(x, w, w0))**2\n",
    "\n",
    "def mean_square_loss(x, y, w, w0):\n",
    "    # the axis=1 and keepdims=True are important when x is a full matrix\n",
    "    return np.mean(square_loss(x, y, w, w0), axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression uses a $\\ell_2$ regularizer, the objective function value for the rigde regression is shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_obj(x, y, w, w0, lam):\n",
    "    return np.mean(square_loss(x, y, w, w0), axis = 1, keepdims = True) + lam * np.linalg.norm(w)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function and Gradients\n",
    "\n",
    "We will now define the functions for the gradients of mean squared loss and ridge objective functions. \n",
    "\n",
    "The mean squared loss function is \n",
    "\n",
    "$$\\Phi(\\pmb{w}) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} -\\pmb{w}^T \\pmb{x}^{(i)}-w_0)^2 $$  \n",
    "\n",
    "The gradient of the mean squared loss is\n",
    "\n",
    "$$\\nabla_{\\pmb{w}}\\Phi = -  \\frac{2}{n}\\pmb{x}^{(i)T} (y^{(i)} -\\pmb{w}^T \\pmb{x}^{(i)}-w_0)$$\n",
    "\n",
    "Note that, we will have separate functions for the gradients of `w` and the offset `w0` as we don't want to add a regularizer to the offset late when we calculate the ridge regression  (check lecture slides)\n",
    "\n",
    "The gradient for the offset is \n",
    "\n",
    "$$\\frac{\\partial\\Phi}{\\partial w_0} =-\\frac{2}{n} \\sum _{i=1}^n (y^{(i)}  -w_0 - \\pmb{w}^T\\pmb{x^{(i)}} ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_square_loss_w(x, y, w, w0):\n",
    "    return -2 * (y - lin_reg(x, w, w0)) * x\n",
    "\n",
    "def d_mean_square_loss_w(x, y, w, w0):\n",
    "    return np.mean(d_square_loss_w(x, y, w, w0), axis = 1, keepdims = True)\n",
    "\n",
    "def d_square_loss_w0(x, y, w, w0):\n",
    "    return -2 * (y - lin_reg(x, w, w0))\n",
    "\n",
    "def d_mean_square_loss_w0(x, y, w, w0):\n",
    "    return np.mean(d_square_loss_w0(x, y, w, w0), axis= 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "The objective function for the ridge regression is follows\n",
    "\n",
    "$$\\Phi_{ridge}(\\pmb{w},w_0) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} -w_0-\\pmb{w}^T \\pmb{x}^{(i)})^2+ \\lambda \\|\\pmb{w}\\|^2$$ \n",
    "\n",
    "The gradient of the ridge function for the weights is\n",
    "\n",
    "$$\\nabla_{\\pmb{w}}\\Phi = -  \\frac{2}{n}\\pmb{x}^{(i)T} (y^{(i)} -w_0-\\pmb{w}^T \\pmb{x}^{(i)}) + 2 \\lambda \\pmb w$$\n",
    "\n",
    "We don't add regularizer for the offset, so the gradient for the offset is\n",
    "\n",
    "$$\\frac{\\partial\\Phi}{\\partial w_0} =-\\frac{2}{n} \\sum _{i=1}^n (y^{(i)}  -w_0 - \\pmb{w}^T\\pmb{x^{(i)}} ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_ridge_obj_w(x, y, w, w0, lam):\n",
    "    return d_mean_square_loss_w(x, y, w, w0) + 2 * lam * w\n",
    "\n",
    "def d_ridge_obj_w0(x, y, w, w0, lam):\n",
    "    return d_mean_square_loss_w0(x, y, w, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent for Linear Regression\n",
    "\n",
    "Implement batch gradient descent for linear regression using the `batch_gd_reg` function below\n",
    "\n",
    "As a reminder batch gradient descent for linear regression is\n",
    "\n",
    "---\n",
    "\n",
    "- Start from an initial point $\\pmb{w}^{(0)} \\in \\mathbb{R}^d$, $w_0^{(0)}\\in \\mathbb{R}$, $t=0$\n",
    "- **for** $i = 1, \\dots, m$ \n",
    "  - $\\pmb{w}^{(i)}=\\pmb{w}^{(i-1)} - \\alpha(i) \\nabla_{\\pmb w}\\Phi(\\pmb{w}^{(i-1)},w_0^{(i-1)})$\n",
    "  - $w_0^{(i)}=w_0^{(i-1)}-\\alpha(i)\\frac{\\partial \\Phi(\\pmb w^{(i-1)},w_0^{(i-1)})}{\\partial w_0}$\n",
    "- **return** $\\pmb{w}^{(i)}$\n",
    "\n",
    "---\n",
    "\n",
    "`batch_gd_reg` function will input: \n",
    "\n",
    "- `X` is the $d \\times n$ features array. We have $d=2$ features and $n=4$ examples of each of those.\n",
    "- `Y` is $1 \\times n$ output values vector\n",
    "- `phi` the objective function for regression\n",
    "- `dphi_w` gradient of the objective function with respect to  `w`\n",
    "- `dphi_w0` partial derivative of the objective function with respect to offset `w0`\n",
    "- `w` is the initial weights\n",
    "- `w0` is the offset value\n",
    "- `m` is the number of maximum iterations\n",
    "- `alpha` is a *function* that takes the iteration index as its input and returns a step size\n",
    "- `lam` is the $\\lambda$ learning rate for the regularizer\n",
    "\n",
    "This function returns a tuple of:\n",
    "\n",
    "- `w` final weights\n",
    "- `w0` final offset\n",
    "- `fs` the list of values of $\\Phi$ (the objective function found during all iterations)\n",
    "- `ws` the list of values of $w$ found during all iterations\n",
    "- `w0s` the list of values of offset values found during all iterations.\n",
    "\n",
    "#### 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd_reg(X, y, phi, dphi_w, dphi_w0, w, w0, max_iter, alpha, lam):\n",
    "    # Lists to store objective function values, weights, and offset values\n",
    "    fs = []\n",
    "    ws = []\n",
    "    w0s = []\n",
    "   \n",
    "    for i in range(max_iter):\n",
    "        # Compute gradients\n",
    "        grad_w = dphi_w(X, y, w, w0, lam)\n",
    "        grad_w0 = dphi_w0(X, y, w, w0, lam)  # Pass lam to the gradient function\n",
    "       \n",
    "        # Update weights\n",
    "        w -= alpha(i) * grad_w\n",
    "        # Update offset\n",
    "        w0 -= alpha(i) * grad_w0\n",
    "       \n",
    "        # Evaluate objective function value\n",
    "        f = phi(X, y, w, w0, lam)\n",
    "        \n",
    "        # Store values\n",
    "        fs.append(f)\n",
    "        ws.append(w.copy())\n",
    "        w0s.append(w0.copy())\n",
    "\n",
    "    return w, w0, fs, ws, w0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can use the `package_ans2` function below, which returns the `w` and `w0` from the last iteration, and the first and last values in `fs`, `ws` and `w0s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_ans2(vals):\n",
    "    w, w0, fs, ws, w0s = vals\n",
    "    return [w.tolist(), w0.tolist(), [fs[0], fs[-1]], [ws[0].tolist(), ws[-1].tolist()], [w0s[0].tolist(), w0s[-1].tolist()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some test cases to learn a least squares and ridge regression model using `batch_gd_reg` function you implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression BGS Test Case 1\n",
      "[[[0.9827167901763396], [0.05354751230684316]], [[0.014375881076711599]], [array([[0.0322163]]), array([[0.03150238]])], [[[0.9983], [0.049490000000000006]], [[0.9827167901763396], [0.05354751230684316]]], [[[-0.0004999999999999994]], [[0.014375881076711599]]]]\n"
     ]
    }
   ],
   "source": [
    "ans = package_ans2(batch_gd_reg(X, Y, ridge_obj, d_ridge_obj_w, d_ridge_obj_w0, w, w0, 1000, lambda i: 0.01, 0.01))\n",
    "print('Ridge Regression BGS Test Case 1')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for Linear Regression\n",
    "\n",
    "Implement stochastic gradient descent with same inputs and outputs. As a reminder, stochastic gradient descent algorithm updates the weights based on the gradient of a random sample from the data at each step. However, batch gradient descent updates the weights based on all samples. Therefore, in gradient step, at each iteration we select a random data instance, and update the weights based on the gradient for that data instance. \n",
    "\n",
    "`sgd_reg` function inputs \n",
    "\n",
    "- `X` is the $n \\times d$ features array. We have 2 features and 4 examples of each of those.\n",
    "- `Y` is $1 \\times n$ output values vector\n",
    "- `phi` the objective function for regression\n",
    "- `dphi_w` gradient of the objective function with respect to  `w`\n",
    "- `dphi_w0` gradient of the objective function with respect to offset `w_0`\n",
    "- `w` is the initial weights\n",
    "- `w0` is the offset value\n",
    "- `m` is the number of maximum iterations\n",
    "- `alpha` is a *function* that takes the iteration index as its input and returns a step size\n",
    "- `lam` is the $\\lambda$ learning rate for the regularizer\n",
    "\n",
    "`sgd_reg` returns a tuple of:\n",
    "\n",
    "- `w` final weights\n",
    "- `w0` final offset\n",
    "- `fs` the list of values of $\\Phi$ (the objective function found during all iterations)\n",
    "- `ws` the list of values of $w$ found during all iterations\n",
    "- `w0s` the list of values of offset values found during all iterations.\n",
    "\n",
    "You can use the function `np.random.randint(n)` to get a random sample from your data at each step of stochastic gradient descent. \n",
    "\n",
    "#### 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_reg(X, y, phi, dphi_w, dphi_w0, w, w0, m, alpha, lam):\n",
    "    fs = []  # List to store objective function values\n",
    "    ws = []  # List to store weight vectors\n",
    "    w0s = []  # List to store offset values\n",
    "    \n",
    "    n = X.shape[1]  # Number of data instances\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Randomly select a single data instance\n",
    "        idx = np.random.randint(n)\n",
    "        x_i = X[:, idx].reshape(-1, 1)\n",
    "        y_i = y[:, idx]\n",
    "        \n",
    "        # Compute gradients for the selected data instance\n",
    "        grad_w = dphi_w(x_i, y_i, w, w0, lam)  # Pass lam to the gradient function\n",
    "        grad_w0 = dphi_w0(x_i, y_i, w, w0, lam)  # Pass lam to the gradient function\n",
    "        \n",
    "        # Update weights and offset\n",
    "        w = w - alpha(i) * grad_w\n",
    "        w0 = w0 - alpha(i) * grad_w0\n",
    "        \n",
    "        # Evaluate objective function value\n",
    "        f = phi(X, y, w, w0, lam)  # Pass the entire dataset for objective function\n",
    "        fs.append(f)\n",
    "        ws.append(w.copy())\n",
    "        w0s.append(w0.copy())\n",
    "\n",
    "    return w, w0, fs, ws, w0s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression SGD Test Case\n",
      "[[[0.9860252663989337], [0.04651738079845325]], [[0.008071062851847489]], [array([[0.0342481]]), array([[0.03154245]])], [[[0.9706233198840081], [0.043312380570250594]], [[0.9860252663989337], [0.04651738079845325]]], [[[0.004247844364732715]], [[0.008071062851847489]]]]\n"
     ]
    }
   ],
   "source": [
    "ans = package_ans2(sgd_reg(X, Y, ridge_obj, d_ridge_obj_w, d_ridge_obj_w0, w, w0, 1000, lambda i: .1/(i+1), 0.01))\n",
    "print('Ridge Regression SGD Test Case')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus - Logistic Regression\n",
    "\n",
    "Implement the batch gradient descent for *logistic regression* as well.\n",
    "\n",
    "You can use the data below for your test cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1., 2., 3., 4.], [1., 1., 1., 1.]])\n",
    "Y_l = np.array([[0, 0, 1, 1]])\n",
    "w = np.array([[1.0],[0.05]])\n",
    "w0 = np.array([[0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(x, w, w0):\n",
    "    z = np.dot(w.T, x) + w0\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(x, y, w, w0):\n",
    "    y_pred = log_reg(x, w, w0)\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_log_obj(x, y, w, w0, lam):\n",
    "    return nll_loss(x, y, w, w0) + lam * np.linalg.norm(w)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, once you implement the functions above, you don't need to make much changes for the gradient of the logistic ridge regression function below:\n",
    "\n",
    "#### 20 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_ridge_log_obj_w(x, y, w, w0, lam):\n",
    "    y_pred = log_reg(x, w, w0)\n",
    "    return np.mean((y_pred - y) * x, axis=1, keepdims=True) + 2 * lam * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_ridge_log_obj_w0(x, y, w, w0, lam):\n",
    "    y_pred = log_reg(x, w, w0)\n",
    "    return np.mean(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic ridge regression test case\n",
      "[[[0.991812891951152], [-0.9714786997724872]], [[-1.1451659661269802]], [0.8995832026832725, 0.3715826763055747], [[[0.9940286960427255], [0.04607904747224163]], [[0.991812891951152], [-0.9714786997724872]]], [[[-0.0039109525277583705]], [[-1.1451659661269802]]]]\n"
     ]
    }
   ],
   "source": [
    "ans = package_ans2(batch_gd_reg(X, Y_l, ridge_log_obj, d_ridge_log_obj_w, d_ridge_log_obj_w0, w, w0, 1000, lambda i: 0.01, 0.01))\n",
    "print('Logistic ridge regression test case')\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
